treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=msplit))
# Plot Decision Tree
rpart.plot(treeCar, box.palette="RdBu", shadow.col="gray",
main=paste(c("Decision Tree - Min Split:", msplit), collapse = ""), nn=TRUE)
#Pridiction
predCar=predict(treeCar,test_set,type="class")
#Matrix
treeCM=table(test_set[,7],predCar)
treeCM
#Accuracy of the prediction using Decision Tree
accuracy=sum(diag(treeCM))/sum(treeCM)
accuracy
library(caret)
library(e1071)
msplit = 140
x=carData[,1:6]
y=carData[,7]
control <- trainControl(method="cv", number=10, savePredictions=TRUE)
seed <- 123
metric <- "Accuracy"
set.seed(seed)
rf_default <- train(x,y, method="rpart", metric=metric, control = rpart.control(minsplit = msplit),
trControl=control)
print(rf_default)
varImp(rf_default)
ggplot(varImp(rf_default))
head(x)
x=carData[,1:5]
head(x)
x=carData[,1:6,-5]
head(x)
x=carData[,.]
head(x)
x=carData[,.]
x=carData[,1:4]
head(x)
x=carData[,-5]
head(x)
x=carData[,-5:-6]
head(x)
x=carData[,-6]
head(x)
x=carData[,-(6:7)]
head(x)
x=carData[,1:6]
head(x)
y=carData[,7]
control <- trainControl(method="cv", number=10, savePredictions=TRUE)
seed <- 123
metric <- "Accuracy"
set.seed(seed)
rf_default <- train(x,y, method="rpart", metric=metric, control = rpart.control(minsplit = msplit),
trControl=control)
print(rf_default)
varImp(rf_default)
ggplot(varImp(rf_default))
carData=read.csv("car.csv")
summary(carData)
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(carData$shouldBuy, SplitRatio = 0.75)
training_set = subset(carData, split == TRUE)
test_set = subset(carData, split == FALSE)
x=training_set[,1:6]
y=training_set[,7]
library(randomForest)
min_range = 100
library(randomForest)
min_range = 100
max_range = 1000
skip = 100
#Defining nTree Range
ntree_list <- c(seq(min_range, max_range, skip))
accuracy_list <- c()
for (i in ntree_list){
rf=randomForest(x,y, ntree=i)
rfp=predict(rf,training_set)
rfCM=table(rfp,training_set[,7])
# rfCM
#Accuracy of the prediction using randomForest
accuracy = sum(diag(rfCM))/sum(rfCM)
accuracy_list <- c(accuracy_list,accuracy)
}
#Plotting the Accuracy of The Training Dataset for Different Min Splits
plot(y=accuracy_list,x=ntree_list,
main = paste(c("Randon Forest - Accuracy of Training Dataset for Different nTree (",min_range,"-",max_range,")"), collapse = ""),
ylab = "Accuracy",
xlab = "nTree Value")
#Displaying the Accuracy of The Training Dataset for Different nTree
ntree_list
accuracy_list
setwd("C:/Users/Meghashyam/Desktop/Data Mining/Assignment 4")
# If you are on linux you can uncomment the following lines to run caret on multiple cores
#install.packages("doMC")
#install.packages("caret")
#library(doMC)
#registerDoMC(4)
library(caret)
getwd()
# xy needs to be tried for 3 data sets.
# 1. 15minutesmatrix.txt
# 2. daymatrix.txt
# 3. hourlymatrix.txt
xy=read.table("15minutesmatrix.txt",sep=' ',header=F)
y=xy[,8]
head(y)
x=xy[,1:7]
myCvControl <- trainControl(method = "repeatedcv",
number = 10)
glmFitTime <- train(V8 ~ .,
data = xy,
method = "glm",
preProc = c("center", "scale"),
tuneLength = 10,
trControl = myCvControl)
print(glmFitTime)
summary(glmFitTime)
summary(glmFitTime)
y_hat = predict(glmFitTime, newdata = x)
summary(y_hat)
mean(100*abs(y_hat-y)/y)
# Support Vector Machine Radial
svmFitTime <- train(V8 ~ .,
data = xy,
method = "svmRadial",
preProc = c("center", "scale"),
#tuneLength = 10,
trControl = myCvControl)
# Neural Network
nnFitTime <- train(V8 ~ .,
data = xy,
method = "avNNet",
preProc = c("center", "scale"),
trControl = myCvControl,
#tuneLength = 10,
linout = T,
trace = F,
MaxNWts = 10 * (ncol(xy) + 1) + 10 + 1,
maxit = 100)
library(rpart)
library(pROC)
library(randomForest)
library(caret)
library(ggplot2)
library(rpart)
library(pROC)
library(randomForest)
library(caret)
library(ggplot2)
set.seed(123)
carData=read.csv("car.csv")
summary(carData)
# View(carData)
x=carData[,1:6]
y=carData[,7]
carData=read.csv("car.csv")
setwd("C:/Users/Meghashyam/Desktop/Data Mining/Assignment 2")
carData=read.csv("car.csv")
summary(carData)
x=carData[,1:6]
y=carData[,7]
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(carData$shouldBuy, SplitRatio = 0.75)
training_set = subset(carData, split == TRUE)
test_set = subset(carData, split == FALSE)
list_data <- c()
library(rpart)
for (i in seq(0, 1000, 10)){
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=i))
#Pridiction
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy
accuracy = sum(diag(treeCM))/sum(treeCM)
list_data <- c(list_data,accuracy)
}
list_data <- c(list_data,accuracy)
for (i in seq(0, 500, 10)){
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=i))
#Pridiction
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy
accuracy = sum(diag(treeCM))/sum(treeCM)
list_data <- c(list_data,accuracy)
}
plot(y=list_data,x=temp_list_data,
main = "Decision Tree - Accuracy measure with Minsplit [0,1000]",
ylab = "Accuracy",
xlab = "Min Split Value")
for (i in seq(0, 1000, 10)){
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=i))
#Pridiction
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy
accuracy = sum(diag(treeCM))/sum(treeCM)
list_data <- c(list_data,accuracy)
}
temp_list_data <- c(seq(0, 1000, 10))
temp_list_data
list_data
plot(y=list_data,x=temp_list_data,
main = "Decision Tree - Accuracy measure with Minsplit [0,1000]",
ylab = "Accuracy",
xlab = "Min Split Value")
library(rpart)
library(pROC)
library(randomForest)
library(caret)
library(ggplot2)
set.seed(123)
carData=read.csv("car.csv")
summary(carData)
# View(carData)
x=carData[,1:6]
y=carData[,7]
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(carData$shouldBuy, SplitRatio = 0.75)
training_set = subset(carData, split == TRUE)
test_set = subset(carData, split == FALSE)
list_data <- c()
library(rpart)
library(pROC)
library(randomForest)
library(caret)
library(ggplot2)
set.seed(123)
carData=read.csv("car.csv")
summary(carData)
# View(carData)
x=carData[,1:6]
y=carData[,7]
# Splitting the dataset into the Training set and Test set
library(caTools)
set.seed(123)
split = sample.split(carData$shouldBuy, SplitRatio = 0.75)
training_set = subset(carData, split == TRUE)
test_set = subset(carData, split == FALSE)
list_data <- c()
library(rpart)
for (i in seq(0, 500, 10)){
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=i))
#Pridiction
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy
accuracy = sum(diag(treeCM))/sum(treeCM)
list_data <- c(list_data,accuracy)
}
temp_list_data <- c(seq(0, 500, 10))
temp_list_data
list_data
plot(y=list_data,x=temp_list_data,
main = "Decision Tree - Accuracy measure with Minsplit [0,1000]",
ylab = "Accuracy",
xlab = "Min Split Value")
library(rpart)
treeCar = rpart(shouldBuy~.,data=training_set,method="class",control=rpart.control(minsplit=140))
treeCar
predCar=predict(treeCar,training_set,type="class")
#Matrix
treeCM=table(training_set[,7],predCar)
treeCM
#Accuracy of the prediction using Decision Tree
accuracy=sum(diag(treeCM))/sum(treeCM)
accuracy
library(ggplot2)
library(GGally)
library(DMwR)
library(factoextra) # clustering algorithms & visualization
library(NbClust)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
set.seed(5580)
setwd("C:/Users/Meghashyam/Desktop/Data Mining/Assignment 1")
cust = read.csv('customercluster.csv')
View(cust)
#Plot scatters plot.
ggpairs(cust[,which(names(cust)!="CustomerID")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Customers Before Outlier Removal")
#Plot scatters plot.
ggpairs(cust.clean[,which(names(cust.clean)!="CustomerID")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Customers After Outlier Removal")
#Scale Data
cust.scale = scale(cust.clean[-1])
#Elbow method gives us an optimal group of clusters.
fviz_nbclust(cust.scale, kmeans, method = "wss") +
geom_vline(xintercept = 6, linetype = 2)+
labs(subtitle = "Elbow method")
#Gap statistic method gives us the count of clusters where the overlap is minimal.
fviz_nbclust(cust.scale, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
labs(subtitle = "Gap statistic method")
set.seed(5580)
pkm_experiment = kmeans(cust.scale, 6, 150)
fviz_cluster(pkm_experiment, data = cust.scale)
set.seed(5580)
pkm = kmeans(cust.scale, 6, 150)
cust.realCenters = unscale(pkm$centers, cust.scale)
clusteredCust = cbind(cust.clean, pkm$cluster)
#View(clusteredCust)
plot(clusteredCust[,2:6], col=pkm$cluster)
write.csv(clusteredCust, file ='customercluster1.csv',col.names = FALSE)
pkm_experiment = kmeans(cust.scale, 6, 150)
#Gap statistic method gives us the count of clusters where the overlap is minimal.
fviz_nbclust(cust.scale, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
labs(subtitle = "Gap statistic method")
#Scale Data
cust.scale = scale(cust.clean[-1])
library(ggplot2)
library(GGally)
library(DMwR)
library(factoextra) # clustering algorithms & visualization
library(NbClust)
library(tidyverse)  # data manipulation
library(cluster)
#Scale Data
cust.scale = scale(cust.clean[-1])
cust.clean <- cust[cust$CustomerID != "0", ]
cust.clean <- cust.clean[cust.clean$CustomerID != "14646", ]
#Scale Data
cust.scale = scale(cust.clean[-1])
#Elbow method gives us an optimal group of clusters.
fviz_nbclust(cust.scale, kmeans, method = "wss") +
geom_vline(xintercept = 6, linetype = 2)+
labs(subtitle = "Elbow method")
#Gap statistic method gives us the count of clusters where the overlap is minimal.
fviz_nbclust(cust.scale, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
labs(subtitle = "Gap statistic method")
set.seed(5580)
pkm_experiment = kmeans(cust.scale, 6, 150)
fviz_cluster(pkm_experiment, data = cust.scale)
set.seed(5580)
pkm = kmeans(cust.scale, 6, 150)
cust.realCenters = unscale(pkm$centers, cust.scale)
clusteredCust = cbind(cust.clean, pkm$cluster)
#View(clusteredCust)
plot(clusteredCust[,2:6], col=pkm$cluster)
write.csv(clusteredCust, file ='customercluster1.csv',col.names = FALSE)
cust.realCenters
getwd()
setwd("C:/Users/Meghashyam/Desktop/Data Mining/Assignment 3")
library(arules)
library(plyr)
df_user= read.csv("temp.csv")
df_user <- df_user[df_user$InvoiceNo != "0", ]
View(df_user)
df_user = ddply(df_user,c("InvoiceNo"),function(dfl)paste(dfl$Description, collapse = ","))
df_user$InvoiceNo = NULL
write.table(df_user,"Milestones2.csv", quote=FALSE, row.names = FALSE, col.names = FALSE)
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
summary(tr)
itemFrequencyPlot(tr, topN=10)
write.table(df_user,"Milestones2.csv", quote=FALSE, row.names = FALSE, col.names = FALSE)
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
install.packages("arulesViz")
library(plyr)
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
install.packages("arules")
library(arules)
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
summary(tr)
itemFrequencyPlot(tr, topN=10)
#-------------------------------------------------------------
#supp = 0.03
rules = apriori(tr,parameter = list(supp=0.03,conf=0.5))
inspect(rules)
#supp = 0.03 (Gives No Rules)
#
#-------------------------------------------------------------
#supp = 0.02
rules = apriori(tr,parameter = list(supp=0.02,conf=0.5))
inspect(rules)
#supp = 0.02 (Gives 17 Rules)
#
#-------------------------------------------------------------
#supp = 0.01
rules = apriori(tr,parameter = list(supp=0.01,conf=0.5))
inspect(rules)
rules.sub = subset(rules, subset = lift > 1 & lift < 10)
inspect(rules.sub)
rules.sub = sort(rules.sub,by='lift')
inspect(rules.sub)
itemsets=unique(generatingItemsets(rules.sub))
itemsets
inspect(itemsets)
maxrules = apriori(tr,list(supp=0.02,conf=0.5, target="maximally frequent itemsets"))
inspect(sort(maxrules))
plot(rules.sub[1:5],method = "graph",control = list(type = "items"))
plot(rules.sub[1:23],method = "matrix",control = list(type = "items",reorder))
#-------------------------------------------------------------
#plotting the graph.
#install.packages("arulesViz")
library(arulesViz)
plot(rules.sub[1:5],method = "graph",control = list(type = "items"))
plot(rules.sub[1:23],method = "matrix",control = list(type = "items",reorder))
arulesViz::plotly_arules(rules.sub)
arulesViz::plotly_arules(rules.sub[1:15])
setwd("C:/Users/Meghashyam/Desktop/Data Mining/Assignment 3")
library(arules)
library(plyr)
df_user= read.csv("temp.csv")
df_user <- df_user[df_user$InvoiceNo != "0", ]
View(df_user)
df_user = ddply(df_user,c("InvoiceNo"),function(dfl)paste(dfl$Description, collapse = ","))
df_user$InvoiceNo = NULL
write.table(df_user,"Milestones2.csv", quote=FALSE, row.names = FALSE, col.names = FALSE)
tr = read.transactions("Milestones2.csv",format="basket",sep=",")
summary(tr)
itemFrequencyPlot(tr, topN=10)
rules = apriori(tr,parameter = list(supp=0.03,conf=0.4))
rules = apriori(tr,parameter = list(supp=0.03,conf=0.4))
inspect(rules)
#supp = 0.03 (Gives No Rules)
#
#-------------------------------------------------------------
#supp = 0.02
rules = apriori(tr,parameter = list(supp=0.02,conf=0.5))
inspect(rules)
#supp = 0.03 (Gives No Rules)
#
#-------------------------------------------------------------
#supp = 0.02
rules = apriori(tr,parameter = list(supp=0.02,conf=0.4))
inspect(rules)
library(ggplot2)
library(GGally)
library(DMwR)
library(factoextra) # clustering algorithms & visualization
library(NbClust)
library(tidyverse)  # data manipulation
library(cluster)
set.seed(5580)
cust = read.csv('customercluster.csv')
setwd("C:/Users/Meghashyam/Desktop/Data Mining/Assignment 1")
library(ggplot2)
library(GGally)
library(DMwR)
library(factoextra) # clustering algorithms & visualization
library(NbClust)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
set.seed(5580)
cust = read.csv('customercluster.csv')
#Plot scatters plot.
ggpairs(cust[,which(names(cust)!="CustomerID")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Customers Before Outlier Removal")
#Scale Data
cust.scale = scale(cust.clean[-1])
set.seed(5580)
pkm_experiment = kmeans(cust.scale, 8, 150)
fviz_cluster(pkm_experiment, data = cust.scale)
#Elbow method gives us an optimal group of clusters.
fviz_nbclust(cust.scale, kmeans, method = "wss") +
geom_vline(xintercept = 6, linetype = 2)+
labs(subtitle = "Elbow method")
#Elbow method gives us an optimal group of clusters.
fviz_nbclust(cust.scale, kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2)+
labs(subtitle = "Elbow method")
cust = read.csv('customercluster.csv')
#Plot scatters plot.
ggpairs(cust[,which(names(cust)!="CustomerID")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Customers Before Outlier Removal")
#Plot scatters plot.
ggpairs(cust[,which(names(cust)!="CustomerID")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Customers Before Outlier Removal")
set.seed(5580)
cust = read.csv('customercluster.csv')
#Plot scatters plot.
ggpairs(cust[,which(names(cust)!="CustomerID")], upper = list(continuous = ggally_points),
lower = list(continuous = "points"), title = "Customers Before Outlier Removal")
#Scale Data
cust.scale = scale(cust.clean[-1])
#Elbow method gives us an optimal group of clusters.
fviz_nbclust(cust.scale, kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2)+
labs(subtitle = "Elbow method")
#Scale Data
cust.scale = scale(cust.clean[-1])
#Elbow method gives us an optimal group of clusters.
fviz_nbclust(cust.scale, kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2)+
labs(subtitle = "Elbow method")
#Scale Data
cust.scale = scale(cust.clean[-1])
library(ggplot2)
library(GGally)
library(DMwR)
library(factoextra) # clustering algorithms & visualization
library(NbClust)
library(tidyverse)  # data manipulation
library(cluster)
cust.scale1 = scale(cust)
#Elbow method gives us an optimal group of clusters.
fviz_nbclust(cust.scale1, kmeans, method = "wss") +
geom_vline(xintercept = 8, linetype = 2)+
labs(subtitle = "Elbow method")
